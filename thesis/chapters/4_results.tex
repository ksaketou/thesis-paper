\chapter*{Results}
\addcontentsline{toc}{chapter}{Results}

In this section we present the results of our Systematic Literature Review and the study of the papers identified. We separately refer to 
each Research Question individually by commenting and making a short reference to the results found. In some sections and always depending on the RQ, we further analyze any 
techniques or practices encountered.

\section {Contribution Type}
After reviewing all of the papers, we ended up with several contribution types proposed in their context. At this point, we should note that some papers 
proposed more than one types so some of them are included into more than one categories. \\

The first and most encountered contribution type is that of a method technique. This type includes any proposal of approaches, processes, techniques and any 
other alternative way of performing a certain task. We grouped them all into this category since they all somehow refer to something very similar. Specifically, 20 of the
studied papers suggest such an approach of that type. 9 out those 20 papers (45\%) propose a way for generating test cases automatically from different input types, like 
use case specification and funtional requirement documents. Motwani et al \cite{8812070}, for example, present Swami which is a language-agnostic test case constructor 
based on Javascript code templates and the ECMA-262 Standard. Swami generates test case code based on the code documentation and software specifications provided. Moreover, 
Nogueira et al \cite{nogueira2015automatic} propose an event flow for test case generation from input provided in a Control Flow Language. This flow consists of several other 
processes (e.g. translation of use case descriptions) that contribute to the achievemennt of the overall goal of the proposal. \\

Another significant contribution type we distinguished is that of a tool. 14 of the studied papers suggest some sort of tool for the ease of different software testing processes. 
Some of those tools are part of the overall approach, whereas others are the main focus of the study. Some of them target the test case execution process, others focus on test case 
generation or even other areas like defects prevention and requirement analysis facilitation. Pedemonte et al \cite{pedemonte2012towards} present an approach to convert manual 
functional tests into a state ready for automatic execution by using Machine Learning methods to process textual information. This is a semi-automatic approach since in cases 
of textual information ambiguity, the user is prompted to interact with the tool to resolve the issue and provide the answer required to continue with the process. Another 
considerable approach is the CTRAS tool created by Hao et al \cite{8811987} which aims at the summarization of duplicate test reports in oprder to make them easily understandable 
by developers. The tool idenitfies similarities in both textual information and screenshots and comprehensibly presents them into a single report. \\

Several other papers focused on developing a Framework to achieve their goal. Some of them refer to the topic of test case generation while others 
emphasize on the earlier stage of requirement analysis and transformation. Lafi et al \cite{9491761} created a framework which consists of different other 
processes in order to generate test cases. First of all, the use case descriptions are being processed in order to create a control flow graph and an NLP table of the 
system under test. Based on those, test paths are then generated which they will finally lead to the test case code. As far as the requirement analysis phase 
is concerned, Viggiato et al \cite{viggiato2022using} created a framework in order to help testers improve the test cases they create. This framework 
accepts test cases in natural language as input and then provides recommendations and proposed changes. More specifically, it can provide recommendations 
for the improvement of the terminology of the test cases, it can identify potentially missing test steps from those test cases and lastly, it can 
prevent test case duplicates since it is able to identify similar test cases with the one which is currently being analyzed. All of the above three 
outputs contribute immensely to the proper and effective forming of test cases by making the testers' job much easier. \\

The last contribution type spotted is the proposal of some sort of strategy. This might be a certain way of handling or working on a specific process which 
could include the use of some specific method or tool. Yet again, the papers we identified serve different kinds of purposes on different stages 
of testing. Wong et al \cite{wong2015dase} propose DASE, a path pruning strategy which aims to improve test case execution and bug detection. It can also prevent 
defects and increase test coverage. It operates by extracting input constraints from program documentation in order to guide and form test execution paths. 
Moreover, Tahvili et al \cite{10.1145/3195538.3195540} proposed a strategy which, given test specifications in natural language, idntifies relationships 
between those test cases and finally, generates a series of proposed tets case scheduling strategies for execution. This is great way to effectively 
prioritize the test case execution process and optimize test case execution. \\

After the analysis of the identified contribution types of the papers studied, we notice that the majority of scientific contributions revovle around 
the proposal of a technique or method of performing a process. This is not something unexpected. Software Testing consists of any major processes, like 
tets case generation, test planning, defect prevention and others, which shape all together the overall purpose of this knowledge area. Thus, the fact that 
many researchers and people interested in the field have figured out ways to enhance those methods makes a lot of sense. They have achieved 
that by proposing new tehniques that be integrated to these processes and give out great results.

\section {Software Testing Stage}
Another aspect of the papers we studied that we are intersted in is the Stage of the Software Testing Lifecycle to which they apply. Some of the approaches we found refer to 
the Requirement Analysis phase, others to the Implementation and Development, Execution etc. The first one we will comment here is the Requirement Analysis. Almost 18\% of the studies 
come up with a proposal regarding that stage and they target to enhance the processing of the test requirements and specifications. This procedure happens before the construction 
of the test cases of any form and it is a critical stage which many times determines the overall outcome of the testing cycle. Thus, it is of major importance to effectivelly 
perform the Requirement Analysis for the optimal result. \\

Sainani et al \cite{reqclass}, for example, propose a method technique which idenitfies and classifies requirements from software engineering contracts. These documents contain all 
the necessary information about the product to be developed, and therefore tested, with the desired characteristics and specifications being a part of them. The developed approach 
analyzes the contract document using different NLP techniques, which we will discuss in a further section, and outputs the different requirements of the product as well as their type. 
The output might add important information regarding the product's architecture or the importance of each feature. Based on that, developers and testers get a clear overview of what 
is worth to be tested more and also the parts that need more attention during the development and testing. This method refers to very early steps on software testing. However, 
this does not mean that such a process is not significant for a successfull testing cycle. On the contrary, it sets a concrete base for all the other stages that follow. \\

Equal importance to the Requirement Analysis stage is given by Femmer et al \cite{femmer2017rapid}. They propose a tool called Smella, which identifies Requirement Smells inside 
Requirement documents. At this point, it is worth mentioning what Requirement Smells actually are. Code smells are parts of code that need to be somehow changed in order to enhance 
its structure, complexity or comprehension \cite{fowler2018refactoring}. Consequently, Requirement Smells represent spots of Requirement Documents which need to be modified in a 
certain way in order for the product's specifications to be stated with the optimal way. For example, in a certain document it is possible to have the same requirement expressed in 
a different way. This might happen due to syntax mistakes or overall use of complex syntax in the document, which makes comprehension a harder task for the reader. This is 
the job of the Smella tool. By making good use of different NLP techniques and incorporating them into the tool, the authors managed to perform the processed described above. \\

The next stage we will refer to in this section is Software Test Planning which contains tasks like Effort Estimation and the overall planning of the roadmap of the testing 
process. Yang et al \cite{9617598} propose a method called DivClass which aims to define the optimal prioritization of test reports inspection in Crowdsource Testing. This testing type 
is covered in a next section of this chapter, so we will not further explain it here. The proposed method initially performs numerous natural language processing tasks to transform the 
test report dataset it accepts as input. Then, the similarity of those test reports is calculated and finally, based on that, the inspection priority of each one is determined. This is a 
short description of the overall method. We perform detailed reference to the different NLP techniques in Section 4.6. This approach can benefit the Software Testing process since it can 
reduce the time spent on the inspection of test reports which are not that critical or that they don't contain high importance information. On the contrary, substantial amount of time 
can be spent on inspecting test reports of higher priority. Another method which targets to make Software Testing faster is the one proposed by Tahvili et al \cite{8051381}. Their approach 
predicts the execution time of manual test cases based on their textual specifications and also on historical data from previously executed test cases. Undoubtedly this approach has a lot 
to offer to the software testing community since it enhances the manual test case execution process which most of the times it ends up requiring the most amount of time to be completed. Taking 
into account that the tester now knows the approximate execution time of the test cases, it is easier for him/her to plan the overall test cycle and create the expected timetable. \\

The testing stage which gathers the majority of the attention in the scinetific literature is the implementation/development stage. That is when the test cases are created and the whole testing 
project comes to life. At this point, it is worth mentioning that almost the 70\% of the papers studied refer to that stage in some way. Because of that, we have a plethora of approach types each 
targeting a different issue. A very common task we frequently came across is the one of test case generation, usually from Requirement Documents expressed in Natural Language. Kamalakar et all 
\cite{kamalakar2013automatically}, for example, created a tool called Kirby which generates test case code from textual Requirement Specifications. Code pieces of the System Under Test 
can be also used in order for the tool to better understand the structure and semantics. However, the framework proposed by Lafi et al \cite{9491761} in order to perform the same task is different. 
This approach uses the Use Case Description of the Use Case Diagram expressed in UML. It is obvious that there is a variety of alternatives that achieve the same goals, which is the test case generation. 
Though this is not the only one we identified at the implementation stage. Viggiato et al \cite{viggiato2022using} propose a framework which aims at improving manual test case descriptions. More specifically, 
this approach accepts the test cases expressed in natural language and generates recommendations for the improvement of those descriptions. The framework recommends test case terminology improvements, 
identifies possible missing steps from those test case and it is also capable of finding potential test cases that already exist in the test code and which are similar to the one provided as input. This 
is a very powerfull approach which makes the test case implementation a piece of cake. Developing test cases based on clear, complete and comprehensive descriptions eliminates the possibility of 
mistakes and makes the whole process much faster. \\

Some of the papers we studied aim to also enhance the stage of test case execution. Pedemonte et al \cite{pedemonte2012towards} created a tool which converts manual test cases into automated ones. 
In their analysis, they emphasize on the importance of the automatic test case execution and that is why they pursuited the implementation of that task. Their approach has the ability of accepting user 
guidance if necessary, but 70\% of the test cases on which they evaluated the tool converted automatically to automated test steps without user guidance. Consequently, we see that the execution stage is also 
considered a significant one for the sucessfull completion of the Software Testing process. \\

After analyzing the testing stages on which the studied papers refer to, we should make a short reference to a less popular issue of the testing world which is that of test report manipulation. Some of the studies 
we encountered, develop approaches that aim to transform test code and outcomes into a more human-friendly format. Hao et al \cite{8811987} created a tool which not only identifies duplicate reports, but it also 
summarizes their content into a more comprehensive report. This process also happens on bug reports. On the other hand, Gonzalez et al \cite{10.1145/3283812.3283819} implemented a tool which performs a process 
opposite to what we have encountered so far in this review. The created tool converts JUnit Assertions into text written in English. Goal of this approach is to contribute to the maintenability, understandability 
and analysis of test code by reducing the existence of complex and difficult to explain code.
