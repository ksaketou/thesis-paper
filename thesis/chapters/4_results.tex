\chapter*{Results}
\addcontentsline{toc}{chapter}{Results}

In this section we present the results of our Systematic Literature Review and the study of the papers identified. We separately refer to 
each Research Question individually by commenting and making a short reference to the results found. In some sections and always depending on the RQ, we further analyze any 
techniques or practices encountered.

\section {Contribution Type}
After reviewing all of the papers, we ended up with several contribution types proposed in their context. At this point, we should note that some papers 
proposed more than one types so some of them are included into more than one categories. \\

The first and most encountered contribution type is that of a method technique. This type includes any proposal of approaches, processes, techniques and any 
other alternative way of performing a certain task. We grouped them all into this category since they all somehow refer to something very similar. Specifically, 20 of the
studied papers suggest such an approach of that type. 9 out those 20 papers (45\%) propose a way for generating test cases automatically from different input types, like 
use case specification and funtional requirement documents. Motwani et al \cite{8812070}, for example, present Swami which is a language-agnostic test case constructor 
based on Javascript code templates and the ECMA-262 Standard. Swami generates test case code based on the code documentation and software specifications provided. Moreover, 
Nogueira et al \cite{nogueira2015automatic} propose an event flow for test case generation from input provided in a Control Flow Language. This flow consists of several other 
processes (e.g. translation of use case descriptions) that contribute to the achievemennt of the overall goal of the proposal. \\

Another significant contribution type we distinguished is that of a tool. 14 of the studied papers suggest some sort of tool for the ease of different software testing processes. 
Some of those tools are part of the overall approach, whereas others are the main focus of the study. Some of them target the test case execution process, others focus on test case 
generation or even other areas like defects prevention and requirement analysis facilitation. Pedemonte et al \cite{pedemonte2012towards} present an approach to convert manual 
functional tests into a state ready for automatic execution by using Machine Learning methods to process textual information. This is a semi-automatic approach since in cases 
of textual information ambiguity, the user is prompted to interact with the tool to resolve the issue and provide the answer required to continue with the process. Another 
considerable approach is the CTRAS tool created by Hao et al \cite{8811987} which aims at the summarization of duplicate test reports in oprder to make them easily understandable 
by developers. The tool idenitfies similarities in both textual information and screenshots and comprehensibly presents them into a single report. \\

Several other papers focused on developing a Framework to achieve their goal. Some of them refer to the topic of test case generation while others 
emphasize on the earlier stage of requirement analysis and transformation. Lafi et al \cite{9491761} created a framework which consists of different other 
processes in order to generate test cases. First of all, the use case descriptions are being processed in order to create a control flow graph and an NLP table of the 
system under test. Based on those, test paths are then generated which they will finally lead to the test case code. As far as the requirement analysis phase 
is concerned, Viggiato et al \cite{viggiato2022using} created a framework in order to help testers improve the test cases they create. This framework 
accepts test cases in natural language as input and then provides recommendations and proposed changes. More specifically, it can provide recommendations 
for the improvement of the terminology of the test cases, it can identify potentially missing test steps from those test cases and lastly, it can 
prevent test case duplicates since it is able to identify similar test cases with the one which is currently being analyzed. All of the above three 
outputs contribute immensely to the proper and effective forming of test cases by making the testers' job much easier. \\

The last contribution type spotted is the proposal of some sort of strategy. This might be a certain way of handling or working on a specific process which 
could include the use of some specific method or tool. Yet again, the papers we identified serve different kinds of purposes on different stages 
of testing. Wong et al \cite{wong2015dase} propose DASE, a path pruning strategy which aims to improve test case execution and bug detection. It can also prevent 
defects and increase test coverage. It operates by extracting input constraints from program documentation in order to guide and form test execution paths. 
Moreover, Tahvili et al \cite{10.1145/3195538.3195540} proposed a strategy which, given test specifications in natural language, idntifies relationships 
between those test cases and finally, generates a series of proposed tets case scheduling strategies for execution. This is great way to effectively 
prioritize the test case execution process and optimize test case execution. \\

After the analysis of the identified contribution types of the papers studied, we notice that the majority of scientific contributions revovle around 
the proposal of a technique or method of performing a process. This is not something unexpected. Software Testing consists of any major processes, like 
tets case generation, test planning, defect prevention and others, which shape all together the overall purpose of this knowledge area. Thus, the fact that 
many researchers and people interested in the field have figured out ways to enhance those methods makes a lot of sense. They have achieved 
that by proposing new tehniques that be integrated to these processes and give out great results.

\section {Software Testing Stage}
Another aspect of the papers we studied that we are intersted in is the Stage of the Software Testing Lifecycle to which they apply. Some of the approaches we found refer to 
the Requirement Analysis phase, others to the Implementation and Development, Execution etc. The first one we will comment here is the Requirement Analysis. Almost 18\% of the studies 
come up with a proposal regarding that stage and they target to enhance the processing of the test requirements and specifications. This procedure happens before the construction 
of the test cases of any form and it is a critical stage which many times determines the overall outcome of the testing cycle. Thus, it is of major importance to effectivelly 
perform the Requirement Analysis for the optimal result. \\

Sainani et al \cite{reqclass}, for example, propose a method technique which idenitfies and classifies requirements from software engineering contracts. These documents contain all 
the necessary information about the product to be developed, and therefore tested, with the desired characteristics and specifications being a part of them. The developed approach 
analyzes the contract document using different NLP techniques, which we will discuss in a further section, and outputs the different requirements of the product as well as their type. 
The output might add important information regarding the product's architecture or the importance of each feature. Based on that, developers and testers get a clear overview of what 
is worth to be tested more and also the parts that need more attention during the development and testing. This method refers to very early steps on software testing. However, 
this does not mean that such a process is not significant for a successfull testing cycle. On the contrary, it sets a concrete base for all the other stages that follow. \\

Equal importance to the Requirement Analysis stage is given by Femmer et al \cite{femmer2017rapid}. They propose a tool called Smella, which identifies Requirement Smells inside 
Requirement documents. At this point, it is worth mentioning what Requirement Smells actually are. Code smells are parts of code that need to be somehow changed in order to enhance 
its structure, complexity or comprehension \cite{fowler2018refactoring}. Consequently, Requirement Smells represent spots of Requirement Documents which need to be modified in a 
certain way in order for the product's specifications to be stated with the optimal way. For example, in a certain document it is possible to have the same requirement expressed in 
a different way. This might happen due to syntax mistakes or overall use of complex syntax in the document, which makes comprehension a harder task for the reader. This is 
the job of the Smella tool. By making good use of different NLP techniques and incorporating them into the tool, the authors managed to perform the processed described above. \\

The next stage we will refer to in this section is Software Test Planning which contains tasks like Effort Estimation and the overall planning of the roadmap of the testing 
process. Yang et al \cite{9617598} propose a method called DivClass which aims to define the optimal prioritization of test reports inspection in Crowdsource Testing. This testing type 
is covered in a next section of this chapter, so we will not further explain it here. The proposed method initially performs numerous natural language processing tasks to transform the 
test report dataset it accepts as input. Then, the similarity of those test reports is calculated and finally, based on that, the inspection priority of each one is determined. This is a 
short description of the overall method. We perform detailed reference to the different NLP techniques in Section 4.6. This approach can benefit the Software Testing process since it can 
reduce the time spent on the inspection of test reports which are not that critical or that they don't contain high importance information. On the contrary, substantial amount of time 
can be spent on inspecting test reports of higher priority. Another method which targets to make Software Testing faster is the one proposed by Tahvili et al \cite{8051381}. Their approach 
predicts the execution time of manual test cases based on their textual specifications and also on historical data from previously executed test cases. Undoubtedly this approach has a lot 
to offer to the software testing community since it enhances the manual test case execution process which most of the times it ends up requiring the most amount of time to be completed. Taking 
into account that the tester now knows the approximate execution time of the test cases, it is easier for him/her to plan the overall test cycle and create the expected timetable. \\

The testing stage which gathers the majority of the attention in the scinetific literature is the implementation/development stage. That is when the test cases are created and the whole testing 
project comes to life. At this point, it is worth mentioning that almost the 70\% of the papers studied refer to that stage in some way. Because of that, we have a plethora of approach types each 
targeting a different issue. A very common task we frequently came across is the one of test case generation, usually from Requirement Documents expressed in Natural Language. Kamalakar et all 
\cite{kamalakar2013automatically}, for example, created a tool called Kirby which generates test case code from textual Requirement Specifications. Code pieces of the System Under Test 
can be also used in order for the tool to better understand the structure and semantics. However, the framework proposed by Lafi et al \cite{9491761} in order to perform the same task is different. 
This approach uses the Use Case Description of the Use Case Diagram expressed in UML. It is obvious that there is a variety of alternatives that achieve the same goals, which is the test case generation. 
Though this is not the only one we identified at the implementation stage. Viggiato et al \cite{viggiato2022using} propose a framework which aims at improving manual test case descriptions. More specifically, 
this approach accepts the test cases expressed in natural language and generates recommendations for the improvement of those descriptions. The framework recommends test case terminology improvements, 
identifies possible missing steps from those test case and it is also capable of finding potential test cases that already exist in the test code and which are similar to the one provided as input. This 
is a very powerfull approach which makes the test case implementation a piece of cake. Developing test cases based on clear, complete and comprehensive descriptions eliminates the possibility of 
mistakes and makes the whole process much faster. \\

Some of the papers we studied aim to also enhance the stage of test case execution. Pedemonte et al \cite{pedemonte2012towards} created a tool which converts manual test cases into automated ones. 
In their analysis, they emphasize on the importance of the automatic test case execution and that is why they pursuited the implementation of that task. Their approach has the ability of accepting user 
guidance if necessary, but 70\% of the test cases on which they evaluated the tool converted automatically to automated test steps without user guidance. Consequently, we see that the execution stage is also 
considered a significant one for the sucessfull completion of the Software Testing process. \\

After analyzing the testing stages on which the studied papers refer to, we should make a short reference to a less popular issue of the testing world which is that of test report manipulation. Some of the studies 
we encountered, develop approaches that aim to transform test code and outcomes into a more human-friendly format. Hao et al \cite{8811987} created a tool which not only identifies duplicate reports, but it also 
summarizes their content into a more comprehensive report. This process also happens on bug reports. On the other hand, Gonzalez et al \cite{10.1145/3283812.3283819} implemented a tool which performs a process 
opposite to what we have encountered so far in this review. The created tool converts JUnit Assertions into text written in English. Goal of this approach is to contribute to the maintenability, understandability 
and analysis of test code by reducing the existence of complex and difficult to explain code.

\section {Software Testing Type}
In this section of our analysis, we focus on the test type that each approach refers to. The first kind of categorization we will perform is based on the two wide groups of manual and automated tests. 
Those two testing approaches are widely used in software testing with each one of them serving a different purpose. Manual Testing is a well-established technique where the tester interacts with the 
application in order to validate and ensure it functions as expected and according to requirements. He performs the steps described in the test cases by manually executing them. However, this is an 
extremely expensive process and requires a big amount of resources with time and money being some of them. This gap fills the use of Automated Testing, where important stages like test generation and execution 
are performed automatically. This process significantly decreases the amount of time needed to test the product and that is the reason why Automated Testing is widely used in Agile development where 
software releases are continuous and more frequent.\\

We will firstly refer to the papers that contribute to the manual testing domain. A significant fact we notice from studying the approaches is that they refer to different phases of testing. With manual 
testing alone being a category of major importance, it is somehow reasonable for it to concern a big part of the software testing community. The Toucan4Test tool proposed by Zhang et al \cite{zhang2014systematic} facilitates 
the process of test cases forming and creation from requirements in natural language. More specifically, this approach automatically generates the test cases from specification documents in order for them to be manually 
executed from the testers. This entails that the above process outputs test cases in a comprehensive format which the executor can easily understand and that he can follow the test steps without any trouble. On 
the other hand, Tahvili et al \cite{8051381} focus on manual test planning. Since we also referred to this approach in a previous section of this chapter, here we will shortly recall the context of the proposed method. 
The authors proposed a technique which predicts the execution time of manual test cases. Such an ability is crucial for the optimized manual test execution since this is a very time consuming process. Thus, 
an effective scheduling of the test cases that are to be executed is more than important.\\

Although manual tests are a widely used testing method, automated testing has significantly grown the past few years and it continues to develop. This is also obvious from the fact that 25 out of all the papers studied 
in this review refer to this testing method. Consequently, these approaches correspond to various contribution types and testing stages. They refer to the automatic creation, forming and execution of test cases from 
different sources like requirement documents or use case diagrams. At this point, we should note that the majority of those techniques are applied during the implementation stage. One such example is the Litmus tool 
created by Dwarakanath et al \cite{litmus} which generates test cases from requirement documents without imposing any restrictions to the input language format or syntax. The tool follows a five step process. After it 
verifies that a sentence of the document is testable, the processing and generation procedure of the test case begins. A similar tool was developed by Rane et al \cite{rane2017automatic}. However, in that case the 
authors focused on optimizing the software testing process during Agile software development.\\

Apart from the above two big groups of manual and automated tests, we identified several other more specific testing types according to which we can further characterize some of the studied approaches. Those types are 
acceptance, integration and crowdsourced tests. The goal of acceptance tests is to determine whether the application meets functionality and usability criteria \cite{artoftesting}. These tests are performed by the 
end users of the application. An example of such approach we identified in the papers is the one proposed by Wang et al \cite{wang2020automatic} which targets the automatic generation of acceptance test cases and also, 
their automatic execution. Thus, the manual effort required is significantly reduced. Moving on to the Integration Testing type, it aims to verify that all the system modules and units communicate correctly 
to perform the functionality required \cite{leung1990study}. This testing type is performed after Unit Testing which, on the other hand, tests each module of the application separately. The technique proposed by Rajaraman et 
al \cite{9197868} targets Integration Testing. More specifically, goal of the approach is to identify interconnections and dependencies between different components of an application in order to optimize Integration Testing. 
The dependencies are determined based on the frequency of use and the importance of each component. Such characteristics exist in the application log files which are also the input of the proposed approach of 
the specific paper.\\

The third testing type we will talk about in this section is Crowdsourced Testing. Crowdsourced Testing has been growing in popularity during the last few years with many platforms having already 
been developed to serve that purpose, such as CrowdSprint\footnote{https://crowdsprint.com/crowdsourced-testing/} and TestBirds\footnote{https://www.testbirds.com/services/quality-assurance/}. 
In Crowdsourced Testing, test tasks are available online for anyone who is willing to perform them and test various software products. When the test task is completed, a test report is submitted 
describing the software's behavior \cite{cui2017should}. A paper that focuses on this testing type is one we have previously encountered in this chapter written by Yang et al \cite{9617598}. The authors 
propose a method called DivClass which prioritizes the test reports coming from Crowdsource testers. Also, similar reports are identified and combined into a signle report for more effective management 
of time during test report inspection.\\

Having expanded on the Software Testing Type characteristic of this review, we notice an overall tendency of the scientific community mainly towards Automation Testing approaches and methods to further 
evolve this practice, which is currently gaining a lot of attention. The next major field of interest is Manual Testing, an older and well established method which continues to being a part of the Software 
Testing process. Lastly, we came up to a number of papers which emphasized on a more specific testing type without making distinctions regarding the above categories of Automated and Manual tests. Such test 
types are Integration, Acceptance and Crowdsourced Testing. However, the approaches proposed there can be utilized during both automated and manual testing.

\section{Input Type}
The input type of the studied approaches is another significant criteria based on which we will characterize them. During our analysis, we notice 
that the nature of the input that each method accepts varies from being some sort of test case code, requirement and use case documents. However, the 
majority of the inputs represent information written in natural language.\\

We will start by referring to the approaches that need test case code in order to perform the functionallity required. The tool proposed by Gonzalez et al \cite{10.1145/3283812.3283819} accepts 
as input the test code of the system under test and in particular the part of it which contains the assertions performed to check different parts of the testing 
process. After the JUnit assertions have been provided to the tool, then a summary of the code is generated. More specifically, the assertion code 
is translated into natural language statements in English. That way, the test case code obtains a comprehensive format and the maintenance 
of the test cases becomes way easier and faster. Another approach which has test case code as one of its accepted inputs is the one proposed by 
Kamalakar et al \cite{kamalakar2013automatically} which we have previously referenced again in this chapter. The goal of the approach is the generation 
of test cases from the textual descriptions of the software's functionallity. However, code pieces of the software under test can be also provided as input 
to the approach in order for the tool to better understand the behavior and expected functions of the application that is being tested.\\

A very frequent input type we came across during our analysis is that of test case descriptions written in natural language. The method proposed 
by Kirinuki et al \cite{9609160} aims to facilitate the maintenance of the automated test scripts where the identification of web elements on 
web pages is necessary. In a web application elements like buttons, text input fields etc. tend to change as the application develops. That 
means that the corresponding test scripts need to constantly adapt to the new changes. However, such a process is very time consuming and 
not effective. Thus, the authors move towards script-free testing by proposing a technique where testing is performed through executable test 
cases in natural language with no need to create test scripts. Arruda et al \cite{arruda2020automation} on the other hand, developed a strategy 
for test script generation which accepts as input the textual test case descriptions. This strategy generates reusable code and is also able 
to adapt and integrate with other test generation frameworks for even better results.\\

The use cases and user stories of the system under test is another common input type referenced into the papers studied. That information can be provided through different forms like diagrams or 
textual documents. However, we noticed that textual descriptions outweigh the rest of the accepted input formats. The method proposed by Nogueira et al \cite{nogueira2015automatic} is one of the many which 
aim at the automatic test case generation. However, its accepted input type is the application's use cases written in a Controlled Natural Language (CNL) proposed by the authors. As stated in the paper, CNL 
is a subset of English that can be processed and translated into a formal language with possibly a different format \cite{nogueira2015automatic}. On the other hand, the approach of Mulla et al \cite{mulla2020potent} 
incorporates the user stories into software testing by specifically focusing on Agile practices. This approach exposes the importance of user story artifacts in the whole Software Testing Lifecycle by 
offering, at the same time, reusability of code, increased test case generation speed and reliability.\\

After the analysis of the different input types of the suggested approaches, we cannot question their variety and diversity. However, there is no doubt that most of them share a common characteristic. The input 
information is usually expressed in natural language in order for the corresponding method or tool to perform the task required. Since the papers we study combine Natural Language Processing Techniques with 
Software Testing, it is more than reasonable for the proposed approaches to make good use of textual information of all sorts. This is another way through which the whole contribution of the NLP knowledge area is evident.

\section{Output Type}
We are now moving on to the output type characteristic based on which we will continue our analysis. The different output types distinguished seem to serve multiple purposes. Several approaches generate 
results ready to be used and to enhance the testing process, whereas others create information which can then act as input into another tool, framework or process. Test reports is another frequently encountered type 
which sometimes is also seen to incorporate bug reporting information.\\

The first output type we will refer to is any form of generated code. The corresponding papers of this output type implement automatic test case generation from different input information and as a result, 
they generate executable test case code. The approach of Mueller et al \cite{inproceedings} does exactly that based on the textual requirements of the application's behavior. This is achieved by firstly 
transforming abstract textual information into a more normalized format (Textual Normal Form --- TNF) and then, the authors make good use of the UML diagram representation where the initial information 
is transferred. The UML diagram is translated into classification trees which finally, generate the output code. At this point, we should note that the generated result uses the SystemVerilog language 
to represent the corresponding outcome. According to Bergeron et al \cite{bergeron2006verification}, ``\emph{SystemVerilog is the first truly industry-standard language to cover design, assertions, 
transaction-level modeling and coverage-driven constrained random verification.}'', meaning that SystemVerilog provides a huge flexibility and support when it comes to working in areas like high-level data types, 
object oriented programming, assertions and others. That way, integration does not act as a barrier when it comes to moving and exchanging knowledge and information related to different areas each.\\

Our analysis now continues with the papers which generate information that can be then provided as input to other processes and frameworks. The method proposed by Li et al \cite{10.1145/3368089.3417067} 
identifies similar test steps by using clustering techniques. The generated output is the created clusters which contain the common test steps. Those clusters can be then taken into advantage in order 
to create or even refactor API test methods. Either by acting as input to another tool or by simply being a usefull source of information to the Test Engineer, this approach achieves the development of 
simpler and clean test code.\\

Different forms of models and diagrams is another frequent output type we came across during our analysis. Some of the corresponding approaches refer to the implementation stage, but the number of methods 
that target Test and Requirement Analysis is equally noticeable. Harmain et al \cite{harmain2000cm} developed a tool called CM-Builder which aims at facilitating the process of test analysis. This tool 
accepts textual information regarding the corresponding software's requirements and then formally presents that information into a UML class diagram, consisting of the necessary object classes of the system. 
This class diagram can be then provided to any Test Analyst for further improvement. Thanks to the above process, the overall analysis of the system to be tested is being both enhanced and accelerated. 
Similar is the goal of the MaramaAIC tool developed by Kamalrudin et al \cite{kamalrudin2017maramaaic}. However, the authors focus much more on the textual requirements improvement, thus after the input 
specification documents have been provided to the tool, the generated output is either a model or diagram containing the idenitfied recommendations. Those recommendations can be proposals to provide better consistency, 
completeness and correctness to the requirements.\\

Having concluded our analysis on the output types of the studied approaches, we see that there are two major types which stand out. Those are the executable test code and any form of model or diagram describing 
some type of information. Approaches that generate test code usually aim at the direct implementation and execution of the test cases and have taken up the mission to accelerate the overall testing process. On the 
other hand, papers that propose small and more detailed tools or methods and target the improvement of one specific stage or procedure, provide small but very usefull improvements to small steps of the testing process. 
Nonetheless, both of the above directions, effectively and in their own way, enhance the testing journey.
